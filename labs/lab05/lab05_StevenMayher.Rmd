---
title: "432 Lab 05"
author: "Steven Mayher"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: true
    number_sections: true
    code_folding: show
    df_print: paged
---

## Setup {-}

```{r setup, message = FALSE}
library(knitr)
library(rmdformats)

## Global options
opts_chunk$set(comment=NA)
opts_knit$set(width=75)
```

## Package Loading {-}

```{r load_packages, message = FALSE, warning = FALSE}
library(here)
library(magrittr)
library(janitor)
library(broom)
library(rsample)
library(naniar)
library(Hmisc)
library(rstanarm)
library(broom.mixed)
library(tidymodels)
library(tidyverse)

theme_set(theme_bw())
```

# Question 1

The following code accomplishes the first step of the question, which is to import the `countyhealthrankings_2017.csv` data into a tibble called `chr17`:


```{r, message = FALSE}
chr17 = read_csv(here("labs/lab05/data", "countyhealthrankings_2017.csv")) 
```


The next step is to clean the data and ensure all categorical variables are redefined as factors:


```{r}
chr17 = chr17 %>% clean_names() %>%
  mutate(rural_cat = as.factor(rural_cat)) %>%
  mutate(race_cat = as.factor(race_cat))

chr17 %>% select(inactive, pct_smokers, pct_65plus, sroh_fairpoor, some_college, rural_cat, race_cat) %>%
  glimpse()
```


The following code demonstrates that a two-way table generated from this data matches what's shown in the question prompt:


```{r}
chr17 %>% tabyl(rural_cat, race_cat) %>% 
  adorn_totals(where = c("row", "col")) %>%
  adorn_title(placement = "combined")
```


Lastly, the following code demonstrates that there is no missing data for any of the predictors or the outcome variable that will be used for models 1 and 2:


```{r}
chr17 %>% select(inactive, pct_smokers, rural_cat, pct_65plus, sroh_fairpoor, some_college, race_cat) %>%
  miss_var_summary()
```


# Question 2

The following code splits the `chr17` data into a training sample consisting of approximately 70% of the counties, and a test sample consisting of approximately 30% of the counties, and ensures that a very similar distribution of `race_cat` across both samples:


```{r}
set.seed(2022)

chr_split <- initial_split(chr17, prop = 0.70, strata = race_cat)
chr_train = training(chr_split)
chr_test = testing(chr_split)
```


The following code demonstrates that there is 2194 counties in the training sample and 942 counties in the test sample:


```{r}
count(chr_train)
count(chr_test)
```


# Question 3

The following code generates the requested Spearman p^2 plot for the data that will be used for model 2 from the `chr_train` training sample:


```{r}
plot(spearman2(inactive ~ pct_65plus + sroh_fairpoor + some_college + race_cat, data = chr_train))
```


From the plot above, it seems that both `sroh_fairpoor` and `some_college` would make excellent choices as potential non-linear terms. As the question requests we select a single predictor, `sroh_fairpoor` will be selected.


# Question 4

The pre-processing recipe for Model 1 has been provided below, which normalizes all predictors, uses indicator variables for all factors, and generates a simple Box-Cox transformation on the `pct_smokers` variable:


```{r}
model1_rec = 
  recipe(inactive ~ pct_smokers + rural_cat, data = chr_train) %>%
  step_BoxCox(pct_smokers) %>%
  step_dummy(rural_cat) %>%
  step_normalize(all_predictors())
```


The pre-processing recipe for Model 2 has been provided below, which normalizes all predictors, uses indicator variables for all factors, and uses an orthogonal quadratic polynomial for the predictor variable identified in Question 3, `sroh_fairpoor`:


```{r}
model2_rec =
  recipe(inactive ~ pct_65plus + sroh_fairpoor + some_college + race_cat, data = chr_train) %>%
  step_poly(sroh_fairpoor, degree = 2) %>%
  step_dummy(race_cat) %>%
  step_normalize(all_predictors())
```


# Question 5

The following code specifies the `lm` modeling engine that will be necessary to create the workflows for both models:


```{r}
lm_model_eng = linear_reg() %>% set_engine("lm")
```


With the `lm` modeling engine specified above under the variable name `lm_model_eng`, this model engine will be used below to generate the workflows for both Models 1 and 2:


```{r}
model1_lm_wf = workflow() %>%
  add_model(lm_model_eng) %>%
  add_recipe(model1_rec)

model2_lm_wf = workflow() %>%
  add_model(lm_model_eng) %>%
  add_recipe(model2_rec)
```


The workflows above for Models 1 and 2, each specified as `model1_lm_wf` and `model2_lm_wf` respectively, have been fit to the training data `chr_train` below:


```{r}
model1_fit = fit(model1_lm_wf, chr_train)

model2_fit = fit(model2_lm_wf, chr_train)
```


The coefficients for Model 1 when fitted to the training data `chr_train` have been provided in the tidy below, and includes both the requested standard errors, and additionally the confidence intervals for additional insight:


```{r}
tidy(model1_fit, conf.int = T) %>%
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 3)
```


As with Model 1, the coefficients for Model 2 when fitted to the training data `chr_train` have been provided in the tidy below, and includes both the requested standard errors, and additionally the confidence intervals for additional insight:


```{r}
tidy(model2_fit, conf.int = T) %>%
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 3)
```


Lastly, as requested the number of observations (nobs), R^2, AIC, and BIC values for both fits have been provided below, and both the sigma and adjusted R^2 squared values have been provided as well for additional context: 


```{r}
model1_fit %>% extract_fit_parsnip() %>%
  glance() %>%
  select(12, 1:3, 8, 9) %>%
  kable(dig = 3)

model2_fit %>% extract_fit_parsnip() %>%
  glance() %>%
  select(12, 1:3, 8, 9) %>%
  kable(dig = 3)
```


Based off of the standards of R^2, AIC, and BIC, Model 2 is the better of the two models, as it has both a higher R^2 value and lower AIC and BIC values than Model 1.


# Question 6

To assess the out-of-sample performance of Models 1 and 2 in the test data `chr_test` on the basis of their correlation-based R-square statistic (rsq), root mean squared error (rmse), and mean absolute error (mae), these metrics need to be selected for with the following function below:

```{r}
mets = metric_set(rsq, rmse, mae)
```


With the metrics set with the `metric_set()` function above, the values can now be calculate for both models. Model 1's values have been calculated below:


```{r}
model1_pred_test = 
  predict(model1_fit, chr_test) %>%
  bind_cols(chr_test %>% select(inactive))

model1_res_test = 
  mets(model1_pred_test, truth = inactive, estimate = .pred) %>%
  kable(digits = 3)

model1_res_test
```


And Model 2's values have been calculated below:


```{r}
model2_pred_test = 
  predict(model2_fit, chr_test) %>%
  bind_cols(chr_test %>% select(inactive))

model2_res_test = 
  mets(model2_pred_test, truth = inactive, estimate = .pred) %>%
  kable(digits = 3)

model2_res_test
```


Based on the above metrics, Model 1 actually performs better in the test data, as Model 1's correlation-based R-square value is higher than Model 2's, and both of Model 1's root mean squared error and mean absolute error are lower than Model 2's.


# Question 7

In order to generate a workflow that uses a Bayesian approach with `stan` to fit Model 2, the following code has been used to first generate the engine that will be needed to accomplish this:

```{r}
prior_dist_int = student_t(df = 1)
prior_dist_preds = normal(0, 5)

stan_model_eng <- linear_reg() %>%
  set_engine("stan",
             prior_intercept = prior_dist_int,
             prior = prior_dist_preds)
```


Using the above engine, the workflow can now be generated as shown below:


```{r}
model2_stan_wf = workflow() %>%
  add_model(stan_model_eng) %>%
  add_recipe(model2_rec)
```


The training data `chr_train` can now be fit to the `stan` workflow above using the following code below:


```{r, message = FALSE}
set.seed(2022)
model2_stan_fit = fit(model2_stan_wf, chr_train)
```


Lastly, the coefficients of the fitted data are shown in the tidy data table below:


```{r}
broom.mixed::tidy(model2_stan_fit, conf.int = T) %>% kable(dig = 3)
```


# Question 8

The following code generates a plot of the coefficients comparing the `stan` fit to the `lm` fit:

```{r}
coefs_lm = tidy(model2_fit, conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate(mod = "lm")

coefs_stan = tidy(model2_stan_fit, conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate(mod = "stan")

coefs_comp = bind_rows(coefs_lm, coefs_stan)

ggplot(coefs_comp, aes(x = term, y = estimate, col = mod, ymin = conf.low, ymax = conf.high)) +
  geom_point(position = position_dodge2(width = 0.4)) +
  geom_pointrange(position = position_dodge2(width = 0.4)) +
  geom_hline(yintercept = 0, lty = "dashed") +
  coord_flip() +
  labs(x = "", y = "Estimate (with 95% confidence interval)",
       title = "Comparing the lm and stan model coefficients")
```

It's difficult to draw any meaningful conclusions from this plot due to how far the intercepts for both models are from 0, however it appears that there may not be a *detectable* difference between the two models in regards to their coefficients. Again though I make this statement with reservations given that much of the detail is lost in this plot.


# Quetion 9

To assess the out-of-sample performance of the `lm` and `stan` fits for Model 2 in the test data `chr_test` on the basis of their correlation-based R-square statistic (rsq), root mean squared error (rmse), and mean absolute error (mae), these metrics need to be selected for with the following function below:

```{r}
mets = metric_set(rsq, rmse, mae)
```


With the metrics set with the `metric_set()` function above, the values can now be calculate for both models. The values for the `lm` fit of Model 2's have been calculated below:


```{r}
model2_pred_test = 
  predict(model2_fit, chr_test) %>%
  bind_cols(chr_test %>% select(inactive))

model2_res_test = 
  mets(model2_pred_test, truth = inactive, estimate = .pred) %>%
  kable(digits = 3)

model2_res_test
```


And Model 2's `stan` fit values have been calculated below:


```{r}
model2_stan_pred_test = 
  predict(model2_stan_fit, chr_test) %>%
  bind_cols(chr_test %>% select(inactive))

model2_stan_res_test = 
  mets(model2_stan_pred_test, truth = inactive, estimate = .pred) %>%
  kable(digits = 3)

model2_stan_res_test
```


Based on the above metrics, I would conclude that there is no detectable difference between the `lm` fit and `stan` fit for Model 2.


# Question 10

The pre-processing recipes for Model 1 and 2 will be regenerated below with the removal of the Box-Cox transformation from Model 1 and the non-linear term from Model 2:

```{r}
model1_lin_rec = 
  recipe(inactive ~ pct_smokers + rural_cat, data = chr_train) %>%
  step_dummy(rural_cat) %>%
  step_normalize(all_predictors())

model2_lin_rec =
  recipe(inactive ~ pct_65plus + sroh_fairpoor + some_college + race_cat, data = chr_train) %>%
  step_dummy(race_cat) %>%
  step_normalize(all_predictors())
```


With the recipes above and the previously generated linear model engine `lm_model_eng`, the new workflows and sub-sequent fits for both models can be generated as follows:


```{r}
model1_lin_lm_wf = workflow() %>%
  add_model(lm_model_eng) %>%
  add_recipe(model1_lin_rec)

model2_lin_lm_wf = workflow() %>%
  add_model(lm_model_eng) %>%
  add_recipe(model2_lin_rec)

model1_lin_fit = fit(model1_lin_lm_wf, chr_train)

model2_lin_fit = fit(model2_lin_lm_wf, chr_train)
```


The number of observations (nobs), R^2, AIC, and BIC values for both fits have been provided below, and both the sigma and adjusted R^2 squared values have been provided as well for additional context: 


```{r}
model1_lin_fit %>% extract_fit_parsnip() %>%
  glance() %>%
  select(12, 1:3, 8, 9) %>%
  kable(dig = 3)

model2_lin_fit %>% extract_fit_parsnip() %>%
  glance() %>%
  select(12, 1:3, 8, 9) %>%
  kable(dig = 3)
```


Both models appear to have identical R^2 values (at least when rounded to 3 digits), however Model 2 appears to have slightly lower (i.e. slightly better) AIC and BIC values compared to Model 1.

In order to assess the out-of-sample performance of these altered versions of Models 1 and 2 in the test data `chr_test` on the basis of their correlation-based R-square statistic (rsq), root mean squared error (rmse), and mean absolute error (mae), we simply need to run the following code to generate the results for Model 1:

```{r}
mets = metric_set(rsq, rmse, mae)

model1_lin_pred_test = 
  predict(model1_lin_fit, chr_test) %>%
  bind_cols(chr_test %>% select(inactive))

model1_lin_res_test = 
  mets(model1_lin_pred_test, truth = inactive, estimate = .pred) %>%
  kable(digits = 3)

model1_lin_res_test
```


And the following code to obtain the results for Model 2:


```{r}
model2_lin_pred_test = 
  predict(model2_lin_fit, chr_test) %>%
  bind_cols(chr_test %>% select(inactive))

model2_lin_res_test = 
  mets(model2_lin_pred_test, truth = inactive, estimate = .pred) %>%
  kable(digits = 3)

model2_lin_res_test
```


Just as was the case with the previous models, based on the above metrics, Model 1 appears to perform better in the test data, as Model 1's correlation-based R-square value is higher than Model 2's, and both of Model 1's root mean squared error and mean absolute error are lower than Model 2's.


# Session Information {-}

```{r}
xfun::session_info()
```